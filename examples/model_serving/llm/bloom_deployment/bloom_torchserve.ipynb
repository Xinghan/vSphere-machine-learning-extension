{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86394b2f-f768-444f-a6c5-356abd173cef",
   "metadata": {},
   "source": [
    "# Pytorch Serve\n",
    "\n",
    "This tutorial assumes that you already have knowledge of the basic concepts of pytorch serve. If you have no the aspect knowledge, please see https://pytorch.org/serve/ website.\n",
    "\n",
    "This tutorial will take you to do bloom-7b1 model inference. Please see https://huggingface.co/bigscience/bloom-7b1 to know more bloom-7b1 model. \n",
    "There is mainly 3 sections to learn how to do model inference.\n",
    "* Load large Huggingface models with constrained resources using accelerate\n",
    "* Start model with torchserve\n",
    "* Run model inference to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56784f41-931f-4956-9fb3-5f0176545abe",
   "metadata": {},
   "source": [
    "## 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5f5055-fe3f-48ef-8555-4510ef35df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2bddffa940>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e7/87/25dd46811431cfc5e8d6ba8c80758cb3131574b271fbf06cf1b691dba8d4/accelerate-0.18.0-py3-none-any.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m31.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
      "Installing collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config and accelerate-launch are installed in '/home/model-server/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-0.18.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pynvml==8.0.4\n",
      "  Downloading pynvml-8.0.4-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: pynvml\n",
      "Successfully installed pynvml-8.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "\n",
    "# fix: pynvml.nvml.NVMLError_FunctionNotFound when start bloom-7b1 model\n",
    "!pip install pynvml==8.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cc870-3288-40b7-b01e-27b369ab8a94",
   "metadata": {},
   "source": [
    "## 1. Prepare model and configurations\n",
    "We have to prepare model MAR file and configuration (config.properties) to start model. Thus, This step will guide you through this process.\n",
    "### 1.1 Download bloom-7b1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6d6e1-dbd5-4236-b573-43b8deb4acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Download_model.py --model_name bigscience/bloom-7b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e1a7e-0b3e-48d6-9ba0-2dcd285314d9",
   "metadata": {},
   "source": [
    "The script prints the path where the model is downloaded as below.\n",
    "\n",
    "model/models--bigscience-bloom-7b1/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/\n",
    "\n",
    "The downloaded model is around 14GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f6d2a-e0ac-4bf2-8bd6-e431bc7306b9",
   "metadata": {},
   "source": [
    "### 1.2 Compress downloaded model\n",
    "\n",
    "Navigate to the path got from the above script. In this example it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb44cd-c4b4-4fdc-aa79-fe58fba72d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd model/models--bigscience-bloom-7b1/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/\n",
    "zip -r ~/model.zip *\n",
    "cd ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf270e2-11aa-4f25-87b8-bb7987d6ae0b",
   "metadata": {},
   "source": [
    "### 1.3 Generate MAR file\n",
    "\n",
    "Navigate up to the directory that have custome_handler.py, model.zip, setup_config.json.\n",
    "* custom_handler.py: codes for model initialization, pre-processing, post-processing, etc.\n",
    "* model.zip: Compressed package of model files (*.bin) should be the checkpoint.\n",
    "* setup_config.json: configurations when loading large huggingface model, Refer: https://huggingface.co/docs/transformers/main_classes/model#large-model-loading\n",
    "\n",
    "**Notice**: Should update parameters in the setup_config.json file according to your device resources. \n",
    "For example, With device_map=\"sequential\", Huggingface accelerate will occupy gpu memory in the order of GPUs; \n",
    "\"max_memory\": {\n",
    "        \"0\": \"32GB\"\n",
    "    }\n",
    "There is only 32GB gpu card, hope to load model into this GPU card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4efebb6-b15b-439f-b17f-a6ae887097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver --model-name bloom --version 1.0 --handler custom_handler.py --extra-files model.zip,setup_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03eb10a-99bf-4d58-9f5f-b597a2d59c27",
   "metadata": {},
   "source": [
    "You will see the bloom.mar file once the process executed finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0e66b-fd73-4073-ab5a-4978c086727c",
   "metadata": {},
   "source": [
    "## 2. Start model with torchserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff430dd0-0868-4363-9c73-8e465ac7facb",
   "metadata": {},
   "source": [
    "Move the Mar file to the specific directory\n",
    "\n",
    "Update config.properties, especially notice model_store directory that need to find the MAR file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc7baab-583c-4fd9-aab3-c99d60bbead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchserve/model_store/*.mar\n",
    "!mv bloom.mar torchserve/model_store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b11e63c-1479-40e7-9b9f-19d6e295e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting torchserve/config/config.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile torchserve/config/config.properties\n",
    "inference_address=http://0.0.0.0:8080\n",
    "management_address=http://0.0.0.0:8081\n",
    "metrics_address=http://0.0.0.0:8082\n",
    "enable_envvars_config=true\n",
    "install_py_dep_per_model=true\n",
    "number_of_gpu=1\n",
    "load_models=all\n",
    "max_response_size=655350000\n",
    "default_response_timeout=6000\n",
    "model_store=/home/model-server/bloom/torchserve/model_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33aa0175-5ef3-4685-a9f8-c9cd95b06caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3041736\n",
      "drwxr-xr-x 2 model-server model-server       4096 Apr 18 02:13 .\n",
      "drwxr-xr-x 5 model-server model-server       4096 Apr 18 02:12 ..\n",
      "-rw-r--r-- 1 model-server model-server 3114721892 Apr 18 02:06 bloom.mar\n",
      "total 16\n",
      "drwxr-xr-x 3 model-server model-server 4096 Apr 18 02:10 .\n",
      "drwxr-xr-x 5 model-server model-server 4096 Apr 18 02:12 ..\n",
      "drwxr-xr-x 2 model-server model-server 4096 Apr 18 02:10 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 model-server model-server  320 Apr 18 02:12 config.properties\n"
     ]
    }
   ],
   "source": [
    "!ls -al torchserve/model_store/\n",
    "!ls -al torchserve/config/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfba46-ce96-4a76-b662-e9e7b51dde52",
   "metadata": {},
   "source": [
    "**It takes about 2 to 5 minutes to start model, that depends to your device resources.**\n",
    "\n",
    "If you want to see the logs in the real time, you can run the below command in the terminal. and if you run the below command to start model in this notebook server, that might can't see the logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1db478d-ae08-4a29-8ab0-dccdd4996ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchServe is already running, please use torchserve --stop to stop TorchServe.\n"
     ]
    }
   ],
   "source": [
    "!torchserve --start --ncs --ts-config ./torchserve/config/config.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef5213-64d0-4eae-a3ff-416e6e9a371c",
   "metadata": {},
   "source": [
    "## 3. Run model inference to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef3b25-2cd0-43f6-832e-e34add11209f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 1: Request model inference by curl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f8eb0b-e1f5-40e2-ab8f-c47044539190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on\n"
     ]
    }
   ],
   "source": [
    "!cat sample_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800e9dad-d161-4128-b6bf-4779b8afb6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying ::1:8080...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (::1) port 8080 (#0)\n",
      "> PUT /predictions/bloom HTTP/1.1\n",
      "> Host: localhost:8080\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> Content-Length: 54\n",
      "> Expect: 100-continue\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 100 Continue\n",
      "* We are completely uploaded and fine\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 \n",
      "< x-request-id: 5864229e-1bf1-428c-bcb4-b7ba5653779b\n",
      "< Pragma: no-cache\n",
      "< Cache-Control: no-cache; no-store, must-revalidate, private\n",
      "< Expires: Thu, 01 Jan 1970 00:00:00 UTC\n",
      "< content-length: 385\n",
      "< connection: keep-alive\n",
      "< \n",
      "Today the weather is really nice and I am planning on\n",
      "traveling to the mountains on a holiday in the spring time and there is only one good thing :\n",
      "Cavals (a bit like a backpacker backpack on the other hand) are one of the most commonly used transportation methods on the island.\n",
      "* Connection #0 to host localhost left intact\n",
      "My name is Jekyll, my mother is Hester and I am one of the only girls in the class. My sisters are Ellie,"
     ]
    }
   ],
   "source": [
    "!curl -v \"http://localhost:8080/predictions/bloom\" -T sample_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96423a-2426-4061-8815-2132085703a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "846fbbb6-3fd1-448b-ba59-897f9a2deae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./sample_text1.txt') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "display(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b37c41-291d-436f-bcf7-802ca29d3ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows\\nThe Kubeflow project is dedicated to making deployments of machine learning (ML) workflows. It includes three main activities: • Using Kubeflow to define the workflow • Using Kubeflow to write the workflow • Implementing the Kubeflow workflow in a Java application with a Python package This project builds upon the Kubeflow project to support the creation of a Java-based workflow engine using'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Access model endpoint url\n",
    "url='http://localhost:8080/predictions/bloom'\n",
    "\n",
    "def make_request(url,text_content):\n",
    "    response=requests.post(url,text_content)\n",
    "    return response.text\n",
    "   \n",
    "answer=make_request(url,text)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b6fa8-d8bb-41ed-986e-f0c995977a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
